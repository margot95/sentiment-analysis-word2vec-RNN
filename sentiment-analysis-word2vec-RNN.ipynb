{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60354e46",
   "metadata": {},
   "source": [
    "# Sentiment analysis with Word2Vec\n",
    "\n",
    "\n",
    "- Convert words to vectors with Word2Vec\n",
    "- Use the word representation given by Word2vec to feed a RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18a05e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras import layers, Sequential\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e22bc76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-15 12:00:23.003394: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-12-15 12:00:23.003444: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-12-15 12:00:23.003463: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-ETLHI67): /proc/driver/nvidia/version does not exist\n",
      "2022-12-15 12:00:23.003893: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#Load the data\n",
    "def load_data(percentage_of_sentences=None):\n",
    "    train_data, test_data = tfds.load(name=\"imdb_reviews\", split=[\"train\", \"test\"], batch_size=-1, as_supervised=True) #batch_size=-1 will return the full dataset as tf.Tensors and as_supervised = True: the returned tf.data.Dataset will have a 2-tuple structure (input, label)\n",
    "\n",
    "    train_sentences, y_train = tfds.as_numpy(train_data)\n",
    "    test_sentences, y_test = tfds.as_numpy(test_data)\n",
    "    \n",
    "    # Take only a given percentage of the entire data\n",
    "    if percentage_of_sentences is not None:\n",
    "        assert(percentage_of_sentences> 0 and percentage_of_sentences<=100)\n",
    "        \n",
    "        len_train = int(percentage_of_sentences/100*len(train_sentences))\n",
    "        train_sentences, y_train = train_sentences[:len_train], y_train[:len_train]\n",
    "  \n",
    "        len_test = int(percentage_of_sentences/100*len(test_sentences))\n",
    "        test_sentences, y_test = test_sentences[:len_test], y_test[:len_test]\n",
    "    \n",
    "    X_train = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in train_sentences]\n",
    "    X_test = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in test_sentences]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_data(percentage_of_sentences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e99292ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec<vocab=4871, vector_size=30, alpha=0.025>\n"
     ]
    }
   ],
   "source": [
    "#Train a word2vec model on our sentences\n",
    "word2vec = Word2Vec(sentences=X_train, vector_size=30, window =2, min_count=5)\n",
    "print(word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3464757c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert a sentence (list of words) into a matrix representing the words in the embedding space\n",
    "def embed_sentence(word2vec, sentence):\n",
    "    embedded_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in word2vec.wv:\n",
    "            embedded_sentence.append(word2vec.wv[word])\n",
    "        \n",
    "    return np.array(embedded_sentence)\n",
    "\n",
    "# Function that converts a list of sentences into a list of matrices\n",
    "def embedding(word2vec, sentences):\n",
    "    embed = [] \n",
    "    for sentence in sentences:\n",
    "        embedded_sentence = embed_sentence(word2vec, sentence)\n",
    "        embed.append(embedded_sentence)    \n",
    "    return embed\n",
    "\n",
    "# Embed the training and test sentences\n",
    "X_train_embed = embedding(word2vec, X_train)\n",
    "X_test_embed = embedding(word2vec, X_test)\n",
    "\n",
    "\n",
    "# Pad the training and test embedded sentences\n",
    "X_train_pad = pad_sequences(X_train_embed, dtype='float32', padding='post', maxlen=200)\n",
    "X_test_pad = pad_sequences(X_test_embed, dtype='float32', padding='post', maxlen=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9fb8d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking our X_train_pad and X_test_pad, they should be np arrays, 3-dim,  \n",
    "#last dimension must be of the size of the word2vec embedding space, and 1st dim must be of size of X_train and X_test\n",
    "\n",
    "for X in [X_train_pad, X_test_pad]:\n",
    "    assert type(X) == np.ndarray\n",
    "    assert X.shape[-1] == word2vec.wv.vector_size\n",
    "\n",
    "\n",
    "assert X_train_pad.shape[0] == len(X_train)\n",
    "assert X_test_pad.shape[0] == len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3583e8",
   "metadata": {},
   "source": [
    "# Baseline accuracy\n",
    "Here, our 2 labels are balanced, so our baseline accuracy is 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "537dd87a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5048"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_accuracy = y_train.sum() / y_train.shape[0]\n",
    "baseline_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dd0b22",
   "metadata": {},
   "source": [
    "# RNN Model (LSTM) - without transfer learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fef52426",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = word2vec.wv.vectors.shape[0]  #on 5% of the imdb_reviews dataset, that is equal to 4871"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94de7360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model():\n",
    "    model = Sequential()\n",
    "    model.add(layers.Masking())\n",
    "    model.add(layers.LSTM(20, activation=\"tanh\"))\n",
    "    model.add(layers.Dense(10, activation = 'relu'))\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05194ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10/10 [==============================] - 7s 232ms/step - loss: 0.6948 - accuracy: 0.5168\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 2s 242ms/step - loss: 0.6909 - accuracy: 0.5304\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 2s 236ms/step - loss: 0.6891 - accuracy: 0.5536\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 4s 382ms/step - loss: 0.6869 - accuracy: 0.5512\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 4s 393ms/step - loss: 0.6842 - accuracy: 0.5648\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 4s 402ms/step - loss: 0.6801 - accuracy: 0.5688\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 4s 433ms/step - loss: 0.6792 - accuracy: 0.5832\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 4s 422ms/step - loss: 0.6743 - accuracy: 0.5888\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 4s 365ms/step - loss: 0.6712 - accuracy: 0.5872\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 4s 369ms/step - loss: 0.6701 - accuracy: 0.5896\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f91098338e0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callback = EarlyStopping(monitor='accuracy', patience=3)\n",
    "model.fit(X_train_pad, y_train, epochs=10, batch_size=128, callbacks=[callback], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "195eec43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " masking (Masking)           (None, 200, 30)           0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 20)                4080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,301\n",
      "Trainable params: 4,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7e533aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 3s 49ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.6017388 ],\n",
       "       [0.6507066 ],\n",
       "       [0.45992807],\n",
       "       ...,\n",
       "       [0.40447983],\n",
       "       [0.46417248],\n",
       "       [0.49000248]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "747d3e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 3s 49ms/step - loss: 0.6815 - accuracy: 0.5544\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6814813613891602, 0.5544000267982483]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_pad, y_test)\n",
    "#accuracy around 0.54"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da884ae",
   "metadata": {},
   "source": [
    "# Same model - with transfer learning (glove wiki gigaword 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7701daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load pre-trained embeddings from glove-wiki-gigaword-50\n",
    "#chose this model from: print(list(api.info()['models'].keys()))\n",
    "word2vec_transfer = api.load('glove-wiki-gigaword-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68902168",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size_transfer = word2vec_transfer.vector_size\n",
    "vocab_size_transfer = word2vec_transfer.vectors.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "edf8d9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert a sentence (list of words) into a matrix representing the words in the embedding space\n",
    "def embed_sentence_with_TF(word2vec, sentence):\n",
    "    embedded_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in word2vec:\n",
    "            embedded_sentence.append(word2vec[word])        \n",
    "    return np.array(embedded_sentence)\n",
    "\n",
    "# Function that converts a list of sentences into a list of matrices\n",
    "def embedding(word2vec, sentences):\n",
    "    embed = []  \n",
    "    for sentence in sentences:\n",
    "        embedded_sentence = embed_sentence_with_TF(word2vec, sentence)\n",
    "        embed.append(embedded_sentence)   \n",
    "    return embed\n",
    "\n",
    "# Embed the training and test sentences\n",
    "X_train_embed_transfer = embedding(word2vec_transfer, X_train)\n",
    "X_test_embed_transfer = embedding(word2vec_transfer, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "959975c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#padding\n",
    "X_train_pad_transfer = pad_sequences(X_train_embed_transfer, dtype='float32', padding='post', maxlen=200)\n",
    "X_test_pad_transfer = pad_sequences(X_test_embed_transfer, dtype='float32', padding='post', maxlen=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea36d063",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_transfer = init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d562c0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "10/10 [==============================] - 7s 227ms/step - loss: 0.6980 - accuracy: 0.5128\n",
      "Epoch 2/5\n",
      "10/10 [==============================] - 2s 196ms/step - loss: 0.6885 - accuracy: 0.5456\n",
      "Epoch 3/5\n",
      "10/10 [==============================] - 2s 232ms/step - loss: 0.6837 - accuracy: 0.5680\n",
      "Epoch 4/5\n",
      "10/10 [==============================] - 2s 227ms/step - loss: 0.6767 - accuracy: 0.6000\n",
      "Epoch 5/5\n",
      "10/10 [==============================] - 4s 395ms/step - loss: 0.6695 - accuracy: 0.6248\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f90dc9ad210>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_transfer.fit(X_train_pad_transfer, y_train, epochs=100, batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "142a3f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 2s 33ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.5925275 ],\n",
       "       [0.50295913],\n",
       "       [0.48237646],\n",
       "       ...,\n",
       "       [0.5291006 ],\n",
       "       [0.6257042 ],\n",
       "       [0.502998  ]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_transfer.predict(X_test_pad_transfer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c1cdf4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 - 2s - loss: 0.6708 - accuracy: 0.6000 - 2s/epoch - 55ms/step\n"
     ]
    }
   ],
   "source": [
    "res = model_transfer.evaluate(X_test_pad_transfer, y_test, verbose=2)\n",
    "#accuray of more than 0.6\n",
    "#conclusion: because of bigger embedded vocabulary size, better accuracy but also more time for each iteration to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0522532",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
